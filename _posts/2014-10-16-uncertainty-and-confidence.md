
---
layout: tag_post
title: Uncertainty and Confidence
tags: [cog-psych]
---

To have a probability distribution over events is sort of like being infinitely opinionated.

Event 1? Oh, I’ll give it 4% probability. Event 2? Hmm, I think that’s a 10%. Event 3? Wow, that’s unlikely, I’ll give it 0.1%.  You’ve got an opinion about everything under the sun.

If you are highly _uncertain_, your probability distribution is very flat; you believe that lots of things are possible and you aren’t sure which is true.  If you are highly _certain_, your probability distribution is pointy; you believe that one thing is much more likely to be true than the others.

But even if you are very uncertain, very full of doubt, you are still _opinionated_.  You have the opinion that there are many possible correct answers, all about equally likely.  For example, you could believe that the field of possible candidates for President in 2016 is very wide, and argue vigorously against someone who says “No, it’s pretty much sewn up for Hillary Clinton.” To have a flat probability distribution over candidates is to have a distinct belief — the belief that it is _not_ sewn up for Hillary Clinton or anyone else.

To be uncertain is different from the state of _lacking confidence_.

Suppose you suffer a setback.  You fail, publicly, humiliatingly. Your confidence takes a hit. Now, when people ask you your opinion, you say “I don’t know.” Not “well, I’m uncertain, it could be a lot of different things,” but “I don’t have a probability distribution at all. I don’t have hypotheses, I don’t have opinions.”  Of course you shouldn’t have opinions — you’re a failure! It would be socially inappropriate to claim the right to an opinion.

Psychologically, we have a sense of the “unknown” that’s more profoundly blank than mere uncertainty.  The [Ellsberg Paradox](http://en.wikipedia.org/wiki/Ellsberg_paradox) points at this. People will prefer a known risk to an unknown risk, even when this causes them to lose in expected value.  A good Bayesian, when given unknown odds, would have some prior, some guess at what the odds might be, and would act accordingly.  But that’s not what people do in real life. When given unknown odds, people shudder and turn away.

[Knightian uncertainty ](http://en.wikipedia.org/wiki/Knightian_uncertainty)is the attempt to put a mathematical formalism to this kind of lack of confidence.  And once you try to articulate this [rigorously](http://en.wikipedia.org/wiki/Dempster%E2%80%93Shafer_theory), you’ll notice that it requires one to reject the basic axioms of probability — in the case of Dempster-Shafer Theory, you have to reject the assumption of sigma-additivity.

My interpretation is that Knightian uncertainty doesn’t actually make sense.  It leads to predictable ways to construct situations in which your decision theory loses.  It’s a flaw — an _exploitable_ flaw — in human psychology.

The attitude that considers it “arrogant” to “be opinionated” is coming from the model of high confidence and low confidence. People who have earned the right to be confident are allowed to have opinions; everyone else shouldn’t pontificate.

Ordinary probability theory says “but implicitly you have an opinion, or at least a hypothesis, any time you make a decision! You _do_ have an opinion on whether extraterrestrials exist, even if you’ve never explicitly thought about it; you obviously don’t think they’re a serious threat, otherwise you’d be putting a lot more effort into defense against alien invasion.”

In the probabilistic worldview, the “right to an opinion” doesn’t make sense. Everyone, smart or dumb, right or wrong, _has_ working hypotheses; that’s what it means to have a mind at all.  You might still be very uncertain, but you work under uncertainty, you take expected values under uncertainty. You may face uncertain risks, but you make a guess as to how bad they’re likely to be, in order to act at all.  You move in darkness.

It’s not arrogant, in itself. You’re not claiming you’re knowledgeable _relative_ to other people. It’s just impossible _not_ to have a best guess, a thought, however rough.  You have to think _something_, at least until you learn something new and think something else.

The intuition behind Knightian uncertainty really _wants_ there to be a way to insist, “No, really, I don’t _know_ whether there are aliens, I don’t even have a guess, I think nothing, _I can say nothing_.”

And, while I’m not sure exactly why, I find this chilling.
